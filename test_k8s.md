# Оглавление
- [Архитектура OpenShift](#архитектура-openshift)
- [Технические требования к железу на каждый из хостов](#технические-требования-к-железу-на-каждый-из-хостов)
- [Объекты](#объекты)

## Notes and Best practices
* OpenShift в своих контейнерах не может запускать сервисы с портом ниже 1024, а также не может запускать от root. Это сделано в качестве безопасности.
* Шаблонизировать всё
* Использовать всякие тесты. Прежде чем что-то менять в кластере, проверить работает это или нет, или сделать **dry-run/helm template**. "На живую" делать в Проде это плохая практика. **Dry Run** и **Helm Template** - это методы предварительной проверки изменений, которые мы собираемся внести в наш кластер или использовать в Helm charts, соответственно. Они позволяют нам убедиться, что изменения пройдут успешно без непредвиденных последствий.  
  * **Dry Run** - это режим, при котором мы можем выполнить операции на кластере без фактического внесения изменений. Это полезно для того, чтобы убедиться, что наши манифесты правильно составлены и не вызовут ошибок или конфликтов. Пример команды с dry run:  
  ```kubectl apply -f your-deployment.yaml --dry-run=client```
  * **Helm Template** - позволяет нам просмотреть манифесты Kubernetes/OpenShift, которые будет генерировать Helm, без их фактического применения в кластере. Это позволяет нам проверить, как будут выглядеть изменения до применения Helm-чарта. Пример использования Helm Template:  
  ```helm template my-chart ./my-chart-directory```
* При деплое с помощью Helm использовать флаг ```--atomic```, который используется с командой ```helm upgrade``` для обеспечения атомарности обновления **релиза Helm**. Атомарность означает, что либо все изменения успешно применяются, либо ни одно изменение не применяется. Если в ходе обновления возникают какие-либо ошибки или проблемы, то Helm откатывает изменения и сохраняет **предыдущее состояние релиза**. Использование ```--atomic``` позволяет избежать "поломанных" состояний, когда часть изменений применена, а другая часть - нет. Это особенно важно при обновлении критичных систем. Пример использования --atomic: ```helm upgrade --atomic my-release ./my-chart-directory```
* Хорошая практика - вешать лейблы везде и всюду. Создал деплоймент - лейбл повесил, а лучше и не один.
* Использовать мнемокоды (мнемонические имена), чтобы понимать что это и для чего (например имя для секрета, в которым бы хранились данные чтобы спуллить образ, назвать pull-secret)



## Архитектура OpenShift:
  
![architecture](images/architecture.PNG)  
  
Архитектура состоит из:
* **Bootstrap node**
Bootstrap node это понятие, связанное с инициализацией и запуском кластера в системах управления контейнерами и оркестрации, таких как OpenShift и Kubernetes.  
Это временная специализированная нода, которая используется в начальной фазе развертывания нового кластера. Она отвечает за запуск и настройку управляющих компонентов кластера, таких как API-сервер, контроллеры и другие. После завершения инициализации кластера, "bootstrap нода" больше не требуется и может быть удалена.  
Bootstrap node служит для того чтобы, когда master node запускается, они обмениваются между собой сертификатами. В общем и целом это то место откуда master nodes берут сертификаты для API и etcd.

* **Master nodes**  
Это ключевой компонент платформы управления контейнерами и оркестрации, такой как Kubernetes и OpenShift. Мастер-ноды управляют и координируют работой всего кластера, принимая решения о том, как размещать и управлять контейнерами на воркер-нодах.
На мастер нодах находится:
  * **Etcd**  
  Etcd - это распределенное хранилище данных, используемое для хранения конфигурации и состояния кластера. Мастер-ноды поддерживают связь с etcd для хранения и извлечения информации о конфигурации и состоянии объектов.
  * **Controller Manager**  
  Controller Manager - с помощью etcd отслеживает состояние объектов кластера (например, projects, routes, pods, replicasets, services) и поддерживает их в соответствии с желаемым состоянием. Он управляет автоматической реакцией на изменения состояния и выполнением задач контроллера.  
  * **API-сервер**  
  API-сервер - предоставляет интерфейс для управления кластером с помощью API запросов. Через API-сервер администраторы и пользователи могут взаимодействовать с кластером, создавать, управлять и масштабировать ресурсы.
  * **OpenShift Scheduller**  
  OpenShift Scheduller - компонент платформы, который отвечает за распределение запущенных контейнеров (подов) по доступным узлам (нодам) в кластере. Он определяет, на каких узлах следует размещать новые контейнеры с учетом ряда факторов, таких как нагрузка на узлы, ресурсы, требования к доступности и т. д.  

  Мастер-ноды являются "мозгами" кластера, предоставляя управление, координацию и хранение состояния. Они работают в пассивном режиме и следят за состоянием кластера, принимая решения о том, как поддерживать желаемое состояние объектов и как реагировать на изменения.  
  Может состоять и из одной ноды, но Исходя из требований RedHat минимальное количество 3 ноды - это необходимо для того чтобы существовал quorum, т.е. чтобы все виртуальные машины с etcd могли договориться кто же из них главнее. Идеальное количество это 3-5-7 по протоколу рафт (raft term) для обеспечения надежности и избежания split-brain. Таким образом необходимо иметь больше половины всех мастер-нод в состоянии доступности. Например, в кластере из 3 мастер-нод, кворум составляет 2 мастер-ноды, это обеспечивает большинство голосов для принятия решений.

* **Worker nodes**  
Это компоненты кластера, которые предоставляют вычислительные ресурсы для запуска и выполнения контейнеров. Они играют важную роль в оркестрации контейнеров и обеспечивают выполнение контейнеризированных приложений. Они также играют ключевую роль в обеспечении масштабируемости, доступности и управляемости приложений в контейнерной среде.  

На каждой воркер ноде находится:  
* **Контейнерный рантайм**  
Это программное обеспечение, которое позволяет запускать контейнеры на воркер-нодах. В платформах Kubernetes и OpenShift обычно используется Docker, но также могут применяться другие рантаймы, такие как containerd, CRI-O и др.
* **kubelet**  
Kubelet - это агент, который работает на каждой воркер-ноде и управляет жизненным циклом контейнеров. Он отвечает за запуск, остановку и мониторинг контейнеров, а также связь с мастер-нодами для синхронизации состояния.  
Взаимодействует с API-сервером.
* **kubeproxy**  
Kube-proxy - это компонент, который управляет сетевой конфигурацией и маршрутизацией на воркер-ноде. Он обеспечивает балансировку нагрузки и обеспечивает доступность сервисов, размещенных внутри кластера.  
Позволяет по сети подключиться к ресурсам кластера.

* **Pods**  
Pods - это минимальная единица развертывания в Kubernetes и OpenShift. Один или несколько контейнеров могут быть размещены внутри одного пода. Поды работают на воркер-нодах и изолируют контейнеры друг от друга.
![pod](images/pod.PNG)  


## Технические требования к железу на каждый из хостов
**Bootstrap nodes** = 4 cpu, 16 ram, 120hdd  
**Worker nodes** = 2 cpu, 8ram, 120hdd  
**Master nodes** = 4 cpu, 16 ram, 120hdd  



## Объекты
### Deployment и Deployment Config
![deployment](images/deployment.PNG)
Создавая **Deployment**, он создаёт **ReplicaSet** (та сущность, которая следит за количеством реплик пода в кластере), который в свою очередь следит за **pods**, которые ему дали создавать.  
В OpenShift есть объект **Deployment Config**, в котором вместо **ReplicaSet** используется **ReplicationController**.  
**Deployment** просто следит, он создал **pod** (или поды). И если этот pod удалить, **Deployment** это заметит, и он создаст новый pod.  
Т.е. **Deployment** следит за состоянием пода, и при необходимости его создаёт/пересоздаёт, а **ReplicaSet** следит за количеством реплик подов.  
  
#### Разница между Deployment и Deployment Config:  
Эти сущности взаимозаменяемые, но при этом у них несколько разные цели.  
В качестве триггера на изменение **Deployment'а** и **Deployment Config** разные принципы:  
* **Deployment** - если ничего не меняли в **Deployment'е**, то при введённой команде ```helm upgrage --install```, у нас не будет происходить редеплой, потому что переменные не менялись, image не менялся.  
  
**Пример YAML-манифеста Deployment:**  
```apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-app-container
          image: my-app:latest
          ports:
            - containerPort: 8080
```  

* У **Deployment Config** изначально цель следить за нашим image и его состоянием. Мы можем указать в **DeploymentConfig** какой-нибудь image, и мы указываем, что триггер для деплоймента - смена его контрольной суммы у самого image. Как только **DeploymentConfig** увидит, что у image поменялась контрольная сумма, он тут же сделает редеплой - посчитает, что что-то поменялось в нашем приложении.  
В **DeploymentConfig** триггеры служат для перезапуска приложения (расширенные стратегии развертывания) настраиваются с использованием объекта **Triggers**. Один из популярных способов триггеров - это **ImageChangeTrigger**, который позволяет синхронизировать обновление приложения с изменением контейнера Docker.  
  
При создании **DeploymentConfig**, создаётся отдельный под (**Deployment Pod**) с минимальными значениями (100m, 100mib), который следит за состоянием старого **DeploymentConfig** и нового **DeploymentConfig**. Как только новый поднялся, он переходит в состояние ```completed```. Т.е. у **Deployment Pod** цель чтобы предоставить ресурсы для нового **DeploymentConfig** и проследить за его состоянием. Если всё не ок, то он его убивает.  
Также у **DeploymentConfig** есть ```rollout```.  

В этой статье описаны дополнительные объекты ImageStream и BuildConfig https://habr.com/ru/company/innotech/blog/652897/  
Статья о Deployment и Deployment Config https://docs.openshift.com/container-platform/4.10/applications/deployments/what-deployments-are.html  
  
**Пример YAML-манифеста Deployment Config (только для OpenShift):**  
```
apiVersion: apps.openshift.io/v1
kind: DeploymentConfig
metadata:
  name: my-app-deployment-config
spec:
  replicas: 3
  selector:
    app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-app-container
          image: my-app:latest
          ports:
            - containerPort: 8080
  triggers:                               # Настройка триггеров (опционально)
    - type: ImageChange                   # Тип триггера (ImageChange - Этот триггер срабатывает, когда меняется образ контейнера. Вы можете указать, что триггер должен реагировать на изменение тега, контрольной суммы или имени образа. Также есть и другие: ConfigChange - Этот триггер срабатывает, когда меняются конфигурационные данные, связанные с DeploymentConfig. GitHub - Этот триггер интегрируется с репозиторием GitHub и срабатывает при изменении в репозитории. Он может использоваться для автоматического обновления приложения при изменениях в коде. GitLab - Аналогично GitHub триггеру, этот триггер работает с GitLab репозиторием. Generic - Этот триггер предоставляет общий механизм для отправки HTTP POST-запросов по заданному адресу. Он может быть использован для срабатывания на основе пользовательских событий. ImageStreamTag - Этот триггер срабатывает при обновлении конкретного тега в ImageStream. Он может быть полезен, если у вас есть ImageStream и вы хотите, чтобы изменения в образе автоматически обновляли приложение. Manual - Этот триггер позволяет запускать обновление вручную через интерфейс OpenShift)
      imageChangeParams:
        automatic: true                   # Автоматический триггер при изменении image
        containerNames:
          - my-app-container              # Имя контейнера
        from:
          kind: ImageStreamTag
          name: my-app:latest             # Имя image
```  
В этом примере, когда образ с тегом **latest** изменится в ```ImageStream``` с именем **my-app**, триггер будет активирован и DeploymentConfig начнет процесс обновления приложения.  
  
#### Параметры Deployment/Deployment Config:
* **ImagePullPolicy** (```...templates.spec.containers.ImagePullPolicy```) - определяет полиnтику как следует стягивать образ. Принимает одно из следующих значений:
  * ```Always``` - всегда загружать новую версию образа с registry, даже если образ уже существует в локальном кеше. Т.е. cтягивет образ из registry каждый раз при каждом развёртывании.
  * ```IfNotPresent``` - загружать образ только в случае, если он отсутствует в локальном кеше. Если образ уже существует, используется версия из локального кеша (если образ есть на ноде).
  * ```Never``` - никогда не загружать образ из registry. Использовать только локальный кеш.  
Пример манифеста:  
```
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image:latest
      imagePullPolicy: Always  # или IfNotPresent, или Never
```  
Выбор подходящей политики загрузки образа зависит от сценария и требований вашего приложения. Например, при использовании стратегии обновления образов Always может быть полезно, чтобы обеспечивать всегда актуальные версии, но это может повлечь за собой дополнительные загрузки образов. С другой стороны, при использовании IfNotPresent можно сэкономить трафик и ускорить развертывание, но это может привести к использованию устаревших версий образов.  
  
  
### ConfigMap и Secret
![ConfigMap](images/ConfigMap.PNG)  
Если нам необходимо параметризировать наш объект, чтобы образ стартовал с определёнными параметрами (это может быть какой-нибудь файл, переменная окружения, пароль...), или нам необходимо передать одну из переменных (например NGINX_PORT), для этого в Kubernetes/OpenShift есть две сущности - **ConfigMap** и **Secret**  
![ConfigMap1](images/ConfigMap1.PNG)  
  
Для того чтобы использовать **ConfigMap** в нашем **Deployment**, в UI-консоли OpenShift необходимо перейти в **Deployment → Environment** и добавить наш **ConfigMap**.  
Либо можно добавить через **YAML Deployment**  
![ConfigMap2](images/ConfigMap2.PNG)  
* **ConfigMap** - это ресурс в Kubernetes и OpenShift, который используется для хранения конфигурационных данных в виде пар ключ-значение. ConfigMap позволяет вынести конфигурацию из самого приложения и хранить её централизованно в кластере. Это может быть полезно, например, для настройки параметров приложения, переменных окружения, файлов конфигурации и других данных, которые могут быть общими для нескольких контейнеров или подов.  
Пример создания ConfigMap в YAML-формате:  
```
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
data:
  app.properties: |
    key1=value1
    key2=value2
```  
В этом примере создается **ConfigMap** с именем ```my-config```, содержащий конфигурационный файл ```app.properties``` с определенными ключами и значениями.  
**ConfigMap** можно использовать в подах следующим образом:  
```
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image
      envFrom:
        - configMapRef:
            name: my-config
```

* **Secret** - это объекты, которые используются для хранения и управления конфиденциальными данных, такими как пароли, токены, ключи, сертификаты и другая чувствительная информация. Они обеспечивают безопасное хранение и передачу конфиденциальных данных между контейнерами, подами и другими ресурсами в кластере.  
  
| Тип секрета                             | Описание                                                      |
|-----------------------------------------|---------------------------------------------------------------|
| Opaque                                  | Произвольные бинарные данные или строки                       |
| kubernetes.io/service-account-token     | Токены ServiceAccount для аутентификации в API                |
| kubernetes.io/dockerconfigjson          | Конфигурация Docker для аутентификации в реестре Docker       |
| kubernetes.io/basic-auth                | Данные для базовой аутентификации                             |
| kubernetes.io/ssh-auth                  | Ключи SSH для аутентификации                                  |
| kubernetes.io/tls                       | Сертификаты TLS (SSL)                                         |
| bootstrap.kubernetes.io/token           | Токены начальной загрузки для настройки кластера              |

#### Создание секрета из командной строки:
Примечание: в случае с OpenShift в качестве CLI-инструмента используется утилита ```oc```, а не ```kubectl```.  
```kubectl create secret generic my-secret --from-literal=username=myuser --from-literal=password=mypassword```

#### Создание секрета из YAML-файла:  
Создайте файл my-secret.yaml с содержимым:  
```
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
type: Opaque
data:
  username: bXl1c2Vy
  password: bXlwYXNzd29yZA==
```  
Затем примените его: ```kubectl apply -f my-secret.yaml```  
  
#### Получение информации о секретах:  
```
kubectl get secrets
kubectl describe secret my-secret
```  
  
#### Использование секретов в подах:  
Добавьте ссылку на секрет в спецификацию пода:  
```
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image
      env:
        - name: USERNAME                    # Имя переменной окружения
          valueFrom:
            secretKeyRef:
              name: my-secret               # Имя секрета
              key: username                 # Ключ в секрете для значения USERNAME
        - name: PASSWORD                    # Имя переменной окружения
          valueFrom:
            secretKeyRef:
              name: my-secret               # Имя секрета
              key: password                 # Ключ в секрете для значения PASSWORD
```  
Каждый блок valueFrom указывает на разный ключ из секрета my-secret. В данном примере, у вас два ключа в секрете: username и password.  
  
#### Удаление секрета:  
```kubectl delete secret my-secret```  
  
#### Создание секрета:  
* Создаём generic секрет (тип Opaque) с именем my-secret и добавляет в него две пары ключ-значение с использованием флага --from-literal. 
```kubectl create secret generic my-secret --from-literal=username=newuser --from-literal=password=newpassword```  
  
* Создаём секрет для docker registry (тип kubernetes.io/dockerconfigjson):  
```kubectl create secret docker-registry my-docker-secret --docker-server=<registry-server> --docker-username=<your-username> --docker-password=<your-password> --docker-email=<your-email>```
  
* Создаём секрет для TLS (тип kubernetes.io/tls):  
```kubectl create secret tls my-tls-secret --cert=path/to/cert.crt --key=path/to/key.key```

#### Обновление секрета:  
Если нужно создать секрет и применить ему без остановки кода, необходимо в в конец одной из команд указанных в пункте создания секретов добавить следующие флаги:  
```--dry-run=client -o yaml | kubectl apply -f ```  
  
#### Просмотр данных секрета:  
Чтобы просмотреть данные секрета в декодированном виде:  
```kubectl get secret my-secret -o jsonpath='{.data}' | base64 --decode```  
Но проще это сделать так: ```kubectl describe secret my-secret ```

После этого, сгенерированный YAML-конфигурационный файл передается через канал kubectl apply -f -, чтобы применить его и создать секрет в кластере.


### Service 
**Service** - объект Kubernetes/OpenShift, который позволяет получить доступ по TCP, UDP или HTTP из кластера к нашему поду в рамках кластера (за кластером никто не знает ничего).  
  
![service](images/service.PNG)  
  

**Service** как объект смотрит на лейблы.  
Для того чтобы связать **Service** и наш **pod**, их селекторы (блок ```spec.selector```) должны быть равны (т.е. **селекторы == лейблам**).  
В **Deployment** используем **labels** (блок ```spec.template.metadata.labels```), а в **Service** используем **selector** (блок ```spec.selector```).  
<!-- В **Service** лейблы называются селекторами.  -->
  
**Service может быть разных типов:** 
* **ClusterIP** - Чаще всего мы работаем именно с этим типом. В данном случае, сервис как объект обращается к IP кластера (```spec.type: ClusterIP; spec.ClusterIP: <IP```>)  
Тип сервиса **ClusterIP** создает внутренний виртуальный IP-адрес для доступа к сервисам внутри кластера. Этот IP-адрес доступен только внутри кластера и не доступен извне. Service **ClusterIP** предоставляет уровень абстракции, который позволяет подам и другим ресурсам внутри кластера взаимодействовать друг с другом, используя этот виртуальный IP.
* **NodePort** - почти никогда не используется, так как это может быть не безопасно, не надёжно, и невозможно контролировать - это выделенные рандомные порты (предоставляет доступ к нашим приложениям через статический порт на каждой ноде в кластере. Каждая нода будет прослушивать указанный порт и перенаправлять трафик к соответствующему сервису внутри кластера).
* **LoadBalancer** - он может быть как он внешнего производителя, так и от Kubernetes, где более гибкие настройки балансировки между нашими подами. Штатно это RoundRobin - т.е. кто свободен тот и будет переключать трафик пода.
Тип сервиса **LoadBalancer** предоставляет балансировщик нагрузки, который распределяет трафик между подами нашего приложения. Он также позволяет настроить внешний доступ к нашему приложению, обеспечивая балансировку нагрузки между несколькими нодами кластера.  
* **ExternalName** -  предоставляет виртуальное DNS-имя (которое мы задаём в манифесте) для обращения к внешнему ресурсу (сервису) изнутри кластера. Это позволяет вашим приложениям использовать стандартный механизм DNS-разрешения для доступа к внешним ресурсам.  
* **Headless** - Создает сервис без виртуального IP-адреса. Вместо этого, этот тип сервиса предоставляет список DNS-имен подов, связанных с этим сервисом (через селектор).
  
Когда мы работает с сервисом, нужно понимать, что этот объект **Service** позволяет подключаться к нашему поду, к нашему контейнеру по портам которые мы указали. Но это работает только в рамках кластера.  
  
**Важно!**
В рамках одного namespace (project), в рамках одного кластера, мы можем обращаться к соседним подам через **Service** (например, ```curl example```, где example это имя сервиса).  
А объекты типа **Routes** придуманы для того, чтобы мы вне кластера обращались к тому или иному поду или контейнеру. **Route** смотрит на сервис.  
Внутри контейнера мы можем обращаться как по имени **сервиса**, так и по имени **роута**.  
  
  
  
#### Лейблы (labels) и селекторы (selectors)
Это ключевые концепции в оркестраторах контейнеров, таких как Kubernetes и OpenShift. Они предоставляют механизм для организации и идентификации ресурсов в кластере, что облегчает управление, поиск и сегментацию ресурсов.  

**Labels** - это пары ключ-значение, которые можно присвоить объектам в кластере, таким как Pods, Services, ReplicaSets и другие. Лейблы не имеют предопределенного назначения и могут использоваться для определения характеристик, свойств или категорий объектов. Примеры лейблов: ```"environment=production"```, ```"app=web"```, ```"tier=frontend"```. Лейблы добавляют метаданные к объектам и позволяют классифицировать их по различным критериям.  

**Selectors** - это выражения, которые позволяют выбирать объекты на основе их лейблов. Селекторы используются для определения, какие объекты должны быть выбраны для выполнения определенных действий, таких как **балансировка нагрузки, размещение на узлах и т. д**. Селекторы могут быть созданы на основе конкретных лейблов и их значений. Например, селектор ```"environment=production"``` будет выбирать все объекты с лейблом ```"environment"``` и значением ```"production"```.  
Также селектор, может выбирать поды на основе разных лейблов. Пример: Предположим, у нас есть несколько подов, каждый из которых имеет следующие лейблы:  
* ```app=web``` - поды, отвечающие за веб-сервер.
* ```app=api``` - поды, отвечающие за сервер API.
* ```environment=production``` - поды, работающие в продакшн окружении.  
Пример селектора, который выбирает поды на основе разных лейблов:  
```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: example-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
      environment: production
```  
  
В этом примере selector выбирает только поды с лейблами ```app=web``` и ```environment=production```. То есть, **ReplicaSet** будет управлять репликами только тех подов, которые имеют оба эти лейбла одновременно.  
Это может быть полезно, например, когда мы хотим масштабировать только веб-сервера, работающие в production окружении, оставив другие типы подов без изменений.  
  
Таким образом, применение лейблов и селекторов включает:
* **Группировку и организацию:** Лейблы позволяют группировать и организовывать ресурсы в кластере, упрощая управление и поиск.
* **Балансировку нагрузки:** Селекторы используются для определения, на какие воркер-ноды размещать поды, обеспечивая балансировку нагрузки.
* **Управление ресурсами:** Селекторы позволяют определить, какие поды должны быть масштабированы или удалены в зависимости от текущей нагрузки.
* **Поддержание высокой доступности:** Лейблы и селекторы могут использоваться для размещения реплицированных подов на различных воркер-нодах для обеспечения высокой доступности.
* **Разграничение окружений:** Лейблы позволяют разграничить объекты по окружениям (например, test, staging, prodaction).
  
Использование лейблов и селекторов дает возможность более гибко и эффективно управлять и масштабировать ресурсы в кластере, а также упрощает автоматизацию и оркестрацию приложений.  

### Route
**Route** - это объект, который обеспечивает доступ к сервисам, работающим внутри кластера, извне кластера. **Route** позволяет настроить внешний доступ к нашим приложениям, предоставляя внешний URL (HTTP/HTTPS), который клиенты могут использовать для взаимодействия с нашими сервисами.  
  
**Основные характеристики и функции Route:**
* **Внешний доступ:** Route предоставляет механизм для доступа к сервисам внутри кластера через внешний URL, который можно использовать извне кластера, например, из Интернета.
* **Терминация SSL:** Route может обеспечивать терминацию SSL (шифрование) для безопасной передачи данных между клиентами и нашими сервисами.
* **Многоподдоменные маршруты:** Вы можете настроить множество доменов для одного маршрута, что полезно, когда у вас есть несколько поддоменов для разных частей приложения.
* **Интеграция с сервисами:** Route интегрируется с сервисами в нашем кластере, позволяя перенаправлять трафик к нужным сервисам.
* **Сервисное открытие:** Route позволяет открывать доступ к наших сервисам, даже если они находятся за NAT или внутри внутренней сети.
* **Обеспечение доступности:** Route обеспечивает балансировку нагрузки и высокую доступность, направляя трафик к нескольким экземплярам нашего сервиса.  
  
Таким образом **Route** - это важный инструмент для обеспечения доступности и безопасности нашего приложения, когда оно доступно извне кластера.  

**Важно понимать, что есть 2 сущности:**
1. **Ingress** - родная сущность от Kubernetes, которая также поддерживается OpenShift.
2. **Route** - сущность RedHat'а.  
В OpenShift рекомендуется всегда создавать роуты.  
Если создать **ingress** как сущность доступа вне кластера в кластер, то автоматически создастся сущность **route**. Т.е. создали одно, а получилось две - в этом случае лучше сразу создавать **route** без **ingress**.  
  
![Ingress](images/Ingress.PNG)  
  
Если мы хотим снаружи подключиться к нашему приложению (которое находится в pod, который находится в кластере, и к которому уже есть **Service**) - мы создаём **Route**.  
**Route** ведет на кластер OpenShift, где её слушает сущность **Service**, который смотрит на **selector pods**, тот смотрит какие поды есть, и отдаёт нам, например, Hello-World во всю страницу.  
  
**Route** можно создавать из меню **Routes**, но также это можно делать через **Ingresses**.  
  
**Важно!**
В рамках одного namespace (project), в рамках одного кластера, мы можем обращаться к соседним подам через **Service** (например, ```curl example```, где example это имя сервиса).  
А объекты типа **Routes** придуманы для того, чтобы мы вне кластера обращались к тому или иному поду или контейнеру. **Route** смотрит на сервис.  
Внутри контейнера мы можем обращаться как по имени **сервиса**, так и по имени **роута**.  
  
**Роуты** и **Ингрессы** могут использовать разные провайдеры, но чаще всего используются **HAproxy**. Можно использовать **Nginx**, можно другие на **baremetal**. Эти все провайдеры позволяют определить в себе настройки.
  
  
## Что происходит когда запускается под?
Первым делом когда мы либо мы присылаем yaml конфиг в API OpenShift, либо вводим команду, у нас происходит общение с API кластера, затем сам кластер обращается к **Kube Controller Manager**, который смотрит на работу всех нод кластера - worker nodes, master nodesЮ смотрит какие ноды доступны для того чтобы на них можно было завести поды, какие из нод содержат какие-нибудь селекторы и т.д. Т.е. происзводится множество проверок того, какая из нод нам подойдёт для того чтобы мы именно на неё задеплоили свой под.  
**Controller Manager** следит за состоянием нод, и если происходят какие-либо сбои, он об этом пишет в **events**, и проблемную ноду можно вывести из работы - так называемо задрейнить (drain a node), т.е. вывести её из работы кластера.  
  
После того как мы определились с нодой, далее **Scheduller** (процесс внутри master node) отслеживает создание пода, и командует той ноде, которая выбрана при помощи **Controller Manager** и проверяет множество факторов:  
* Минимальные рекомендуемые ресурсы, которые необходимы для ваших приложений (**requests** resources и **limits** resources по cpu и memory), это т.н. **Affinity принадлежности** (подробнее можно прочитать в статье https://habr.com/ru/company/otus/blog/576944/) к нодам.  
**Affinity-правила** позволяют нам контролировать, на каких узлах кластера будут размещаться наши поды. Эти правила определяют предпочтения и ограничения для размещения подов на определенных узлах с учетом их атрибутов, как, например, лейблы. Есть два типа affinity-правил: **node affinity** (аффинити узлов) и **pod affinity** (аффинити подов):
  * **Node Affinity (Аффинити Узлов):** С помощью **node affinity** можно настраивать размещение подов на определенных узлах кластера на основе атрибутов этих узлов. Например, вы можете указать, что поды должны запускаться только на узлах с определенными лейблами или аннотациями (**Аннотации** - это метаданные, которые могут быть прикреплены к различным ресурсам, включая pods, services, nodes и другие объекты. Они представляют собой пары ключ-значение и используются для добавления дополнительной информации о ресурсе, которая может быть полезной для различных целей, таких как отладка, мониторинг, анализ и т.д. Прописываются в блоке с метаданными). Пример:  
  ```
  affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: disk-type
            operator: In
            values:
            - ssd
  ```
    * **Pod Affinity (Аффинити Подов):** С помощью **pod affinity** можно контролировать размещение подов на одних узлах вместе с другими подами, которые имеют определенные атрибуты (лейблы или аннотации). То есть можно указать, что определенные поды следует размещать на тех же узлах, где уже есть поды с определенными атрибутами. Пример:  
    ```
    affinity:
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app
              operator: In
              values:
                - web
        topologyKey: "kubernetes.io/hostname"
    ```  
    Оба эти типа аффинити-правил позволяют более точно контролировать размещение подов на узлах в зависимости от их атрибутов. Это может быть полезно для управления нагрузкой, группировки связанных подов на одних узлах или разделения нагрузки между разными типами узлов.
* Использование жёстких дисков
* Всякие лейблы и селекторы

## Probes
**Probes** (или **healthchecks**) в OpenShift и Kubernetes - это механизмы для проверки здоровья контейнеров, запущенных в подах. Они позволяют системе автоматически определять, насколько работоспособен и готов к обработке запросов контейнер.  
    
**Probes** выполняются с определенной периодичностью и с использованием определенной команды или запроса. Если результат проверки соответствует заданным условиям (например, успешный HTTP-запрос с кодом 200), то под считается здоровым. В противном случае, в зависимости от типа проверки, контейнер может быть перезапущен или отключен от балансировщика нагрузки.  

**Существуют три типа probes:**
* **Startup probes**
Это экспериментальная проверка, которая используется для задания времени запуска контейнера. Она может быть полезна, когда контейнер нуждается в длительной инициализации.
* **liveness probes**
Этот тип проверки определяет, готов ли контейнер обрабатывать запросы. Если liveness probe не проходит, то контейнер перезапускается.  
Пример определения проверки здоровья в readiness probe в YAML-файле пода:  
```
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image
      livenessProbe:
        httpGet:                 # Указывает, что для проверки здоровья будет выполнен HTTP-запрос
          path: /healthz         # Путь, по которому будет выполняться запрос
          port: 8080 # Номер порта контейнера для запроса
        initialDelaySeconds: 10  # начальная задержка перед началом выполнения проверок здоровья (10 секунд после запуска контейнера)
        periodSeconds: 15        # периодичность выполнения проверок здоровья (каждые 15 секунд после начала выполнения), но после того как прошла начальная задержка initialDelaySeconds
```
* **readiness probes**
Этот тип проверки определяет, готов ли контейнер принимать трафик. Если под не прошел readiness probe, его изолируют от балансировщика нагрузки.  
Пример определения проверки здоровья в readiness probe в YAML-файле пода:  
```
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image
      readinessProbe:
        httpGet:                 # Указывает, что для проверки здоровья будет выполнен HTTP-запрос
          path: /healthz         # Путь, по которому будет выполняться запрос
          port: 8080             # Номер порта контейнера для запроса
        initialDelaySeconds: 5   # Начальная задержка в секундах. Определяет, сколько времени должно пройти с момента запуска контейнера до того, как начнут выполняться проверки здоровья. Может быть полезна, чтобы дать контейнеру достаточно времени для запуска и инициализации перед тем, как начать проверки
        periodSeconds: 10        # Период проверки в секундах. Определяет, с какой периодичностью будут выполняться проверки здоровья после того, как начнут работу (то, как часто будут выполняться проверки здоровья после начала выполнения контейнера, но после того как прошла начальная задержка initialDelaySeconds)
```  
  
#### При запуске пода стартуют 3 типа контейнеров:
* **Init container**
**Init container** не отображается, но он запускается и ждёт пока этому контейнеру выдадут сеть, чтобы и **kubelet** и **kubeproxy** могли взаимодействовать с этим контейнером. Т.е. этот контейнер поднимается, получает сеть, выдаёт нашему поду и самоуничтожается.  
Если же он не получит адреса, мы увидим сообщение об этом в **events**, например о том, что закончились ip адреса.  
Они предназначены для выполнения предварительных задач или инициализации перед запуском основных контейнеров в поде. Init-контейнеры могут выполнять скрипты инициализации, загружать данные, устанавливать зависимости и выполнять другие действия, которые необходимо выполнить перед началом работы основных контейнеров.  
Init-контейнеры запускаются перед основными контейнерами в поде и выполняются только один раз до завершения. Их задача - подготовить окружение для основных контейнеров.  

* **Pause container** 
**Pause container** (также иногда называемый **infra контейнером**) - это особый служебный контейнер, который существует в каждом поде, это похоже на шаблон или на родительский контейнер, от которых все новые контейнеры в поде наследуют namespaces. Контейнер pause запускается, затем переходит в “спящий режим”. Это контейнер-шаблон, который резервирует namespaces, которые являются общими для всех контейнеров внутри пода.  
Используется внутри подов для организации сетевой изоляции и управления сетевыми пространствами имен.  
Один из способов обеспечения сетевой изоляции между контейнерами внутри одного пода - это использование контейнера Pause. Контейнер Pause создает сетевое пространство имен и сетевой интерфейс, а затем делегирует сетевые пространства имен остальным контейнерам в поде. Это позволяет каждому контейнеру иметь свой уникальный сетевой стек, но все контейнеры в поде могут взаимодействовать друг с другом через localhost.  
Важно понимать, что контейнер Pause сам по себе не выполняет никаких задач и не взаимодействует с приложением, размещенным в поде. Его основная функция - обеспечить сетевую изоляцию между контейнерами в поде.  
Этот подход позволяет более эффективно использовать ресурсы хостовой системы и предоставлять приложениям внутри одного пода уровень изоляции, аналогичный изоляции, которая была бы между разными подами.  
Pause контейнер запускается вместе с основными контейнерами и продолжает работать в фоновом режиме, обеспечивая сетевую изоляцию между контейнерами в поде.  
Когда стартует **Pause container**, он нам арендует ip, и далее стартует наш под.  
  
* **Основные (Main) контейнеры**  
Это контейнеры, которые выполняют основные задачи нашего приложения. Они могут быть веб-серверами, микросервисами или любыми другими компонентами нашего приложения, которые предоставляют функциональность пользователям или системе.


**Таким образом в нашем поде есть 3 типа контейнеров, которые стартуют в следующей последовательности:**
1. **Init container** - используются для каких-то вещей, которые нужно выполнить до старта нашего основного приложения - например, чтобы запустилась БД, чтобы фронт был доступен. Однако чаще всего это используется для того чтобы из vault'а вытащить логины и пароли, помещает их в переменные окружения и передаёт нашему основному контейнеру. Данный тип контейнеров запускается раньше всех - раньше нашего основного (main), раньше сайдакара (**sidecar** - шаблон проектирования и архитектурный паттерн, который предполагает добавление дополнительного контейнера к основному контейнеру в одном поде. Основная идея состоит в том, чтобы разделить разные функции или задачи приложения на отдельные контейнеры, которые могут работать параллельно в рамках одного пода. **Sidecar-контейнеры** обычно взаимодействуют с основным контейнером и предоставляют дополнительную функциональность или сервисы. Они могут выполнять задачи, такие как сбор логов, мониторинг, авторизация, сетевые прокси, обновление данных и другие. Каждый sidecar-контейнер работает независимо от основного контейнера, но может использовать общие ресурсы и обмениваться данными с другими контейнерами в том же поде).  
Выполняются только один раз до их завершения и самоуничтожаются. Их задача - подготовить окружение для основных контейнеров.  
2. **Main (основные) контейнеры** - после успешного завершения всех init-контейнеров, параллельно запускаются основные контейнеры (main). Эти контейнеры представляют собой основную рабочую нагрузку приложения. 
3. **Pause container** - После запуска основных контейнеров, OpenShift/Kubernetes также создает pause container, который предоставляет сетевое пространство имен и сетевой интерфейс для обеспечения изоляции сетевого взаимодействия между контейнерами в поде - получает и предоставляет IP.  
Pause контейнер запускается вместе с основными контейнерами и продолжает работать в фоновом режиме, обеспечивая сетевую изоляцию между контейнерами в поде.  
  
## Стратегии деплоя (deployment strategies)
Подробно в документации https://docs.openshift.com/container-platform/3.11/dev_guide/deployments/deployment_strategies.html  
**Существуют различные стратегии деплоя:**  
* **RollingUpdate**  
(```spec.strategy.type: RollingUpdate```) - это то что используется в основном. Описывается в объектах Deployment или DeploymentConfig, и она означает, что - мы говорим Kubernetes/OpenShift, что сперва у нас поднимаются новые поды, проходят все проверки, проходит трафик, а потом гасятся старые. В случае с этим типом стратегии даунтайм минимальный.  
Т.е. новые поды постепенно создаются, а старые постепенно удаляются, обеспечивая плавное обновление. Можно настроить параметры обновления, такие как количество одновременно обновляемых подов и период обновления. Пример манифеста:  
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  strategy:
    type: RollingUpdate       # Используем стратегию RollingUpdate
    rollingUpdate:
      maxSurge: 1             # Максимальное количество дополнительных (новых) подов во время обновления
      maxUnavailable: 1       # Максимальное количество недоступных (старых) подов во время обновления. Этот параметр определяет максимальное количество подов, которые могут быть недоступными (старыми) во время выполнения обновления
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-container
          image: my-image:v2  # Используем новую версию образа
```
* **Recreate**  
При **Recreate** сразу удаляется старый под и поднимается новый. При этом типе стратегии будет даунтайм.  
Все старые поды уничтожаются, а затем создаются новые поды с обновленными образами. Пример манифеста:  
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recreate-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  strategy:
    type: Recreate            # Используем стратегию Recreate
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-container
          image: my-image:v2  # Используем новую версию образа
```
* **Bluegreen**  
**Bluegreen** - стратегия развертывания, при которой у нас есть два набора окружений: "синее" и "зеленое". В текущем окружении (например, "синем") находится текущая версия нашего приложения, работающая в продакшн, а в новом окружении (например, "зеленом") разворачивается новая версия. Когда новая версия приложения считается готовой к продакшн, мы переключаем трафик с текущего окружения на новое. Пример: Представим, у вас есть веб-приложение, которое работает в окружении "синее". Вы разработали новую версию и развернули ее в окружении "зеленое". Затем вы переключаете трафик с "синего" на "зеленое", чтобы пользователи начали использовать новую версию. Если что-то пойдет не так, вы можете легко откатиться к "синему" окружению и вернуть работу старой версии.  
Другое определение - Часть подов поднимаются с одним образом (новым), а часть с другим (старым). Это может быть полезно, когда лишняя секунда простоя будет фатальной для бизнеса. Т.е. частично деплоить и постепенно увеличивать кол-во экземпляров с новой версией образа.  
**Bluegreen** нецелесобразно использовать в архитектуре, где также требуется обновить БД, т.к. структура БД меняется при каждом релизе, и если 2 реплики будут работать с одной структурой, а 2 с другой, у нас всё развалится. В тех проектах где БД нет, или нет привязки к структуре, не нужна **миграция liquibase**, это сработает.  
Таким образом, Blue-Green Deployment подразумевает мгновенное переключение между окружениями, в то время как Canary Deployment предполагает постепенное внедрение новой версии для ограниченной аудитории.   
При использовании стратегии "Blue-Green" создаются два независимых деплоймента, представляющих разные версии приложения. Это не совсем разные сервисы, но это разные наборы подов, представляющие разные версии приложения. Один из наборов подов обозначается как "синий" (blue), а другой - как "зеленый" (green). После успешного развертывания новой версии и завершения тестирования, мы переключаем трафик на новую версию, делая ее "зеленой".
Создается новый набор подов (зеленые), которые тестируются, прежде чем старый набор (синие) уничтожается. Пример манифеста:  
```
# Синий деплоймент
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue-deployment             # Имя деплоймента синей версии
  namespace: blue-namespace         # Размещение в пространстве имен blue-namespace
spec:
  replicas: 3                       # Желаемое количество реплик подов
  selector:
    matchLabels:
      app: my-app                   # Селектор по метке app=my-app
      version: blue                 # Селектор по метке version=blue
  template:
    metadata:
      labels:
        app: my-app                 # Метка для пода: app=my-app
        version: blue               # Метка для пода: version=blue
    spec:
      containers:
        - name: my-app-container    # Имя контейнера
          image: my-app:blue        # Используемый образ: my-app:blue
          ports:
            - containerPort: 8080   # Открываем порт 8080

# Сервис для синего деплоймента
---
apiVersion: v1
kind: Service
metadata:
  name: blue-service                # Имя сервиса для синей версии
  namespace: blue-namespace # Размещение в пространстве имен blue-namespace
spec:
  selector:
    app: my-app                     # Селектор по метке app=my-app
    version: blue                   # Селектор по метке version=blue
  ports:
    - protocol: TCP
      port: 80                      # Порт, на который маршрутизируется трафик. Указывает на порт, на который маршрутизируется внешний трафик к сервису снаружи. То есть, когда кто-то отправляет запрос на порт 80 сервиса, этот трафик будет перенаправлен на под с контейнером, который слушает порт 8080 внутри.
      targetPort: 8080              # Порт, на котором работает под. Указывает на порт, на котором запущено приложение внутри контейнера. Когда сервис маршрутизирует трафик к поду, он направляет его на этот порт.

# Зеленый деплоймент
apiVersion: apps/v1
kind: Deployment
metadata:
  name: green-deployment            # Имя деплоймента зеленой версии
  namespace: green-namespace        # Размещение в пространстве имен green-namespace
spec:
  replicas: 3                       # Желаемое количество реплик подов
  selector:
    matchLabels:
      app: my-app                   # Селектор по метке app=my-app
      version: green                # Селектор по метке version=green
  template:
    metadata:
      labels:
        app: my-app                 # Метка для пода: app=my-app
        version: green              # Метка для пода: version=green
    spec:
      containers:
        - name: my-app-container    # Имя контейнера
          image: my-app:green       # Используемый образ: my-app:green
          ports:
            - containerPort: 8080   # Открываем порт 8080

# Сервис для зеленого деплоймента
---
apiVersion: v1
kind: Service
metadata:
  name: green-service               # Имя сервиса для зеленой версии
  namespace: green-namespace        # Размещение в пространстве имен green-namespace
spec:
  selector:
    app: my-app                     # Селектор по метке app=my-app
    version: green                  # Селектор по метке version=green
  ports:
    - protocol: TCP
      port: 80                      # Порт, на который маршрутизируется трафик. Указывает на порт, на который маршрутизируется внешний трафик к сервису снаружи. То есть, когда кто-то отправляет запрос на порт 80 сервиса, этот трафик будет перенаправлен на под с контейнером, который слушает порт 8080 внутри.
      targetPort: 8080              # Порт, на котором работает под. Указывает на порт, на котором запущено приложение внутри контейнера. Когда сервис маршрутизирует трафик к поду, он направляет его на этот порт.
```  
В этом случае, после успешного тестирования, мы можем переключить трафик на "зеленый" деплоймент, обновив настройки балансировщика нагрузки или маршрута (Route) на "зеленый" сервис.

* **Canary Deployment (конареечный релиз)**  
**Canary Deployment** - стратегия развертывания, при которой вы постепенно и поочередно вводите новую версию приложения для небольшой группы пользователей (например, 5% трафика), чтобы оценить стабильность и производительность новой версии. Если новая версия успешно проходит проверку, вы постепенно увеличиваете долю трафика, направляемого на новую версию. Пример: Вы разработали новую функциональность для вашего приложения и хотите удостовериться, что она работает корректно и не вызывает проблем. Вы развертываете новую версию и настраиваете так, чтобы только 5% пользователей получали доступ к новой функциональности. Если в течение определенного времени нет проблем, вы увеличиваете долю до 10%, затем до 20% и так далее. Если появляются проблемы, вы можете быстро ограничить распространение новой версии.  
Таким образом новая версия приложения (например, 10%) постепенно тестируется, а затем, при успешном тестировании, количество экземпляров новой версии увеличивается. Пример манифеста:  
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: canary-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  strategy:
    type: Canary             # Используем стратегию Canary
    canary:
      traffic: 10%           # Отправляем 10% трафика на поды с новой версией
      maxSurge: 1            # Максимальное количество дополнительных (новых) подов для конарейки
      maxUnavailable: 1      # Максимальное количество недоступных (старых) подов для конарейки
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: my-container
          image: my-image:v2   # Используем новую версию образа для конарейки
```  
**Bluegreen** и **конарейка** обычно реализуются при помощи bash, python и прочих "костылей". Не является функцией по умолчанию.  
  
## Operators (Операторы)
**Операторы** - это специальные пользовательские **контроллеры** в Kubernetes и OpenShift (**контроллеры** представляют собой одну из ключевых компонент управления кластером. Они отвечают за поддержание желаемого состояния ресурсов и их автоматическое управление в соответствии с заданными спецификациями), разработанные для автоматизации сложных операций жизненного цикла приложений и сервисов. Они используют принципы контроллеров Kubernetes (Примеры контроллеров в Kubernetes и OpenShift включают в себя **ReplicaSet, Deployment, StatefulSet, DaemonSet, Job и другие**. Каждый контроллер обладает определенной функциональностью и целями, но общим для них является управление жизненным циклом и состоянием ресурсов в кластере), чтобы расширить и настроить возможности управления и автоматизации в кластере.  
Операторы определяются на уровне **CustomResourceDefinition (CRD)**, который расширяет схему Kubernetes, добавляя новые ресурсы, определенные пользователем. Операторы следят за изменениями в созданных **CRD** и на основе спецификаций ресурсов выполняют различные действия, такие как управление жизненным циклом, масштабирование, обновление и т.д.  
Операторы позволяют автоматизировать сложные задачи, такие как установка и настройка приложений, баз данных, мониторинг, а также соблюдение определенных политик и многое другое.  
  
Основная задача оператора сделать что-то после того как что-то произошло в вашем кластере. Например, у вас задеплоился микросервис, оператор увидел, что у вас новые поды, и например, подцепил к ним дополнительные секреты.  
Проще говоря, операторы это функциональная возможность Kubernetes, которая расширяет возможности Kubernetes до любых функций.  
  
Например, существует популярный оператор **flux** - это из серии GitOps'а (например, когда происходит изменение в GIT, а мы видим результат в OpenShift).  
Пример его работы - нам нужно что-то создавать в Kubernetes при помощи GIT, например Namespace или daemonset - мы добавляем оператора, мы его подключаем к отдельному GIT-репозиторию, в котором лежат YAML-файлы, в которых, например указаны Namespace'ы. Мы же заходим в этот GIT-репозиторий, добавляем себе желаемый Namespace, а далее оператор flux видит, что было изменение в GIT, и применяет само у себя (без всяких GitLab/TeamCity) в API Kubernetes и у нас появляется Namespace.  
  
Есть также всякие shell-операторы. Есть оператор для service mesh, и многие другие.  
  
  
## Автомасштабирование (Autoscaling)
Изменение количества ресурсов, выделяемых для приложения, на основе текущей нагрузки. Это позволяет приложению эффективно использовать ресурсы, адаптироваться к изменениям нагрузки и обеспечивать надежную и отзывчивую работу в любых условиях.
#### Существует два основных типа автомасштабирования:
* **Горизонтальное автомасштабирование или HPA (Horizontal Pod Autoscalers)** - при этом типе добавляются или убираются экземпляры приложения для распределения нагрузки. Например, в Kubernetes это может быть изменение количества реплик в Deployment или Replication Controller.  
Т.е. в нём можно описать правила по которым будет меняться количество экземпляров. Например, если нагрузка на CPU становится выше 50%. Также можно указать минимальное и максимальное количество подов (т.е. если не будет нагрузки, если в deployment указано 2 реплики, а в HPA указана 1 реплика, то количество экземпляров снизится до 1)  
* **Вертикальное автомасштабирование или VPA (Vertical Pod Autoscaler)** - в этом случае меняется количество ресурсов (например, CPU и RAM), выделяемых для каждого экземпляра приложения. Например, в Kubernetes это может быть изменение ресурсных запросов и лимитов в манифесте пода.  
  
Автомасштабирование может быть реализовано на основе различных метрик нагрузки, таких как загрузка CPU, использование памяти, количество запросов в секунду и другие.  
Система автомасштабирования наблюдает за этими метриками и, при достижении заданных пороговых значений, автоматически изменяет количество ресурсов или экземпляров, чтобы соответствовать текущей нагрузке.  
Преимущества автомасштабирования включают более эффективное использование ресурсов, обеспечение высокой отзывчивости и доступности приложения, а также уменьшение операционных затрат на мониторинг и реагирование на изменения нагрузки.  
  
  
## Переменные окружения
Переменные можно передавать в под различными способами:
1. Непосредственно в самом deployment'е (https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/)
2. Брать их из секрета (https://kubernetes.io/docs/concepts/configuration/secret/)
3. Брать их из конфигмапа (https://kubernetes.io/docs/concepts/configuration/configmap/)
