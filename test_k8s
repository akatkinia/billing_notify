# Оглавление
- [Архитектура OpenShift](#архитектура-openshift)
- [Технические требования к железу на каждый из хостов](#технические-требования-к-железу-на-каждый-из-хостов)
- [Объекты](#объекты)

## Notes
OpenShift в своих контейнерах не может запускать сервисы с портом ниже 1024, а также не может запускать от root. Это сделано в качестве безопасности.

## Архитектура OpenShift:
  
![architecture](images/architecture.PNG)  
  
Архитектура состоит из:
* **Bootstrap node**
Bootstrap node это понятие, связанное с инициализацией и запуском кластера в системах управления контейнерами и оркестрации, таких как OpenShift и Kubernetes.  
Это временная специализированная нода, которая используется в начальной фазе развертывания нового кластера. Она отвечает за запуск и настройку управляющих компонентов кластера, таких как API-сервер, контроллеры и другие. После завершения инициализации кластера, "bootstrap нода" больше не требуется и может быть удалена.  
Bootstrap node служит для того чтобы, когда master node запускается, они обмениваются между собой сертификатами. В общем и целом это то место откуда master nodes берут сертификаты для API и etcd.

* **Master nodes**  
Это ключевой компонент платформы управления контейнерами и оркестрации, такой как Kubernetes и OpenShift. Мастер-ноды управляют и координируют работой всего кластера, принимая решения о том, как размещать и управлять контейнерами на воркер-нодах.
На мастер нодах находится:
  * **Etcd**  
  Etcd - это распределенное хранилище данных, используемое для хранения конфигурации и состояния кластера. Мастер-ноды поддерживают связь с etcd для хранения и извлечения информации о конфигурации и состоянии объектов.
  * **Controller Manager**  
  Controller Manager - с помощью etcd отслеживает состояние объектов кластера (например, projects, routes, pods, replicasets, services) и поддерживает их в соответствии с желаемым состоянием. Он управляет автоматической реакцией на изменения состояния и выполнением задач контроллера.  
  * **API-сервер**  
  API-сервер - предоставляет интерфейс для управления кластером с помощью API запросов. Через API-сервер администраторы и пользователи могут взаимодействовать с кластером, создавать, управлять и масштабировать ресурсы.
  * **OpenShift Scheduller**  
  OpenShift Scheduller - компонент платформы, который отвечает за распределение запущенных контейнеров (подов) по доступным узлам (нодам) в кластере. Он определяет, на каких узлах следует размещать новые контейнеры с учетом ряда факторов, таких как нагрузка на узлы, ресурсы, требования к доступности и т. д.  

  Мастер-ноды являются "мозгами" кластера, предоставляя управление, координацию и хранение состояния. Они работают в пассивном режиме и следят за состоянием кластера, принимая решения о том, как поддерживать желаемое состояние объектов и как реагировать на изменения.  
  Может состоять и из одной ноды, но Исходя из требований RedHat минимальное количество 3 ноды - это необходимо для того чтобы существовал quorum, т.е. чтобы все виртуальные машины с etcd могли договориться кто же из них главнее. Идеальное количество это 3-5-7 по протоколу рафт (raft term) для обеспечения надежности и избежания split-brain. Таким образом необходимо иметь больше половины всех мастер-нод в состоянии доступности. Например, в кластере из 3 мастер-нод, кворум составляет 2 мастер-ноды, это обеспечивает большинство голосов для принятия решений.

* **Worker nodes**  
Это компоненты кластера, которые предоставляют вычислительные ресурсы для запуска и выполнения контейнеров. Они играют важную роль в оркестрации контейнеров и обеспечивают выполнение контейнеризированных приложений. Они также играют ключевую роль в обеспечении масштабируемости, доступности и управляемости приложений в контейнерной среде.  

На каждой воркер ноде находится:  
* **Контейнерный рантайм**  
Это программное обеспечение, которое позволяет запускать контейнеры на воркер-нодах. В платформах Kubernetes и OpenShift обычно используется Docker, но также могут применяться другие рантаймы, такие как containerd, CRI-O и др.
* **kubelet**  
Kubelet - это агент, который работает на каждой воркер-ноде и управляет жизненным циклом контейнеров. Он отвечает за запуск, остановку и мониторинг контейнеров, а также связь с мастер-нодами для синхронизации состояния.  
Взаимодействует с API-сервером.
* **kubeproxy**  
Kube-proxy - это компонент, который управляет сетевой конфигурацией и маршрутизацией на воркер-ноде. Он обеспечивает балансировку нагрузки и обеспечивает доступность сервисов, размещенных внутри кластера.  
Позволяет по сети подключиться к ресурсам кластера.

* **Pods**  
Pods - это минимальная единица развертывания в Kubernetes и OpenShift. Один или несколько контейнеров могут быть размещены внутри одного пода. Поды работают на воркер-нодах и изолируют контейнеры друг от друга.
![pod](images/pod.PNG)  


## Технические требования к железу на каждый из хостов
**Bootstrap nodes** = 4 cpu, 16 ram, 120hdd  
**Worker nodes** = 2 cpu, 8ram, 120hdd  
**Master nodes** = 4 cpu, 16 ram, 120hdd  



## Объекты
### Deployment и Deployment Config
![deployment](images/deployment.PNG)
Создавая **Deployment**, он создаёт **ReplicaSet** (та сущность, которая следит за количеством реплик пода в кластере), который в свою очередь следит за **pods**, которые ему дали создавать.  
В OpenShift есть объект **Deployment Config**, в котором вместо **ReplicaSet** используется **ReplicationController**.  
**Deployment** просто следит, он создал **pod** (или поды). И если этот pod удалить, **Deployment** это заметит, и он создаст новый pod.  
Т.е. **Deployment** следит за состоянием пода, и при необходимости его создаёт/пересоздаёт, а **ReplicaSet** следит за количеством реплик подов.  

#### Разница между Deployment и Deployment Config:  
Эти сущности взаимозаменяемые, но при этом у них несколько разные цели.  
В качестве триггера на изменение **Deployment'а** и **Deployment Config** разные принципы:  
* **Deployment** - если ничего не меняли в **Deployment'е**, то при введённой команде ```helm upgrage --install```, у нас не будет происходить редеплой, потому что переменные не менялись, image не менялся.  
* У **Deployment Config** изначально цель следить за нашим image и его состоянием. Мы можем указать в **DeploymentConfig** какой-нибудь image, и мы указываем, что триггер для деплоймента - смена его контрольной суммы у самого image. Как только **DeploymentConfig** увидит, что у image поменялась контрольная сумма, он тут же сделает редеплой - посчитает, что что-то поменялось в нашем приложении.  
При создании **DeploymentConfig**, создаётся отдельный под (**Deployment Pod**) с минимальными значениями (100m, 100mib), который следит за состоянием старого **DeploymentConfig** и нового **DeploymentConfig**. Как только новый поднялся, он переходит в состояние ```completed```. Т.е. у **Deployment Pod** цель чтобы предоставить ресурсы для нового **DeploymentConfig** и проследить за его состоянием. Если всё не ок, то он его убивает.  
Также у **DeploymentConfig** есть ```rollout```.  

В этой статье описаны дополнительные объекты ImageStream и BuildConfig https://habr.com/ru/company/innotech/blog/652897/  
Статья о Deployment и Deployment Config https://docs.openshift.com/container-platform/4.10/applications/deployments/what-deployments-are.html  

### ConfigMap и Secret
![ConfigMap](images/ConfigMap.PNG)  
Если нам необходимо параметризировать наш объект, чтобы образ стартовал с определёнными параметрами (это может быть какой-нибудь файл, переменная окружения, пароль...), или нам необходимо передать одну из переменных (например NGINX_PORT), для этого в Kubernetes/OpenShift есть две сущности - **ConfigMap** и **Secret**  
![ConfigMap1](images/ConfigMap1.PNG)  
  
Для того чтобы использовать **ConfigMap** в нашем **Deployment**, в UI-консоли OpenShift необходимо перейти в **Deployment → Environment** и добавить наш **ConfigMap**.  
Либо можно добавить через **YAML Deployment** (скрин )
![ConfigMap2](images/ConfigMap2.PNG)


### Service 
**Service** - объект Kubernetes/OpenShift, который позволяет получить доступ по TCP, UDP или HTTP из кластера к нашему поду в рамках кластера (за кластером никто не знает ничего).  
  
![service](images/service.PNG)  
  

**Service** как объект смотрит на лейблы.  
Для того чтобы связать **Service** и наш **pod**, их селекторы (блок ```spec.selector```) должны быть равны (т.е. **селекторы == лейблам**).  
В **Deployment** используем **labels** (блок ```spec.template.metadata.labels```), а в **Service** используем **selector** (блок ```spec.selector```).  
<!-- В **Service** лейблы называются селекторами.  -->
  
**Service может быть разных типов:** 
* **ClusterIP** - Чаще всего мы работаем именно с этим типом. В данном случае, сервис как объект обращается к IP кластера (```spec.type: ClusterIP; spec.ClusterIP: <IP```>)  
Тип сервиса **ClusterIP** создает внутренний виртуальный IP-адрес для доступа к сервисам внутри кластера. Этот IP-адрес доступен только внутри кластера и не доступен извне. Service **ClusterIP** предоставляет уровень абстракции, который позволяет подам и другим ресурсам внутри кластера взаимодействовать друг с другом, используя этот виртуальный IP.
* **NodePort** - почти никогда не используется, так как это может быть не безопасно, не надёжно, и невозможно контролировать - это выделенные рандомные порты (предоставляет доступ к нашим приложениям через статический порт на каждой ноде в кластере. Каждая нода будет прослушивать указанный порт и перенаправлять трафик к соответствующему сервису внутри кластера).
* **LoadBalancer** - он может быть как он внешнего производителя, так и от Kubernetes, где более гибкие настройки балансировки между нашими подами. Штатно это RoundRobin - т.е. кто свободен тот и будет переключать трафик пода.
Тип сервиса **LoadBalancer** предоставляет балансировщик нагрузки, который распределяет трафик между подами нашего приложения. Он также позволяет настроить внешний доступ к нашему приложению, обеспечивая балансировку нагрузки между несколькими нодами кластера.  
* **ExternalName** -  предоставляет виртуальное DNS-имя (которое мы задаём в манифесте) для обращения к внешнему ресурсу (сервису) изнутри кластера. Это позволяет вашим приложениям использовать стандартный механизм DNS-разрешения для доступа к внешним ресурсам.  
* **Headless** - Создает сервис без виртуального IP-адреса. Вместо этого, этот тип сервиса предоставляет список DNS-имен подов, связанных с этим сервисом (через селектор).
  
Когда мы работает с сервисом, нужно понимать, что этот объект **Service** позволяет подключаться к нашему поду, к нашему контейнеру по портам которые мы указали. Но это работает только в рамках кластера.  
  
**Важно!**
В рамках одного namespace (project), в рамках одного кластера, мы можем обращаться к соседним подам через **Service** (например, ```curl example```, где example это имя сервиса).  
А объекты типа **Routes** придуманы для того, чтобы мы вне кластера обращались к тому или иному поду или контейнеру. **Route** смотрит на сервис.  
Внутри контейнера мы можем обращаться как по имени **сервиса**, так и по имени **роута**.  
  
  
  
#### Лейблы (labels) и селекторы (selectors)
Это ключевые концепции в оркестраторах контейнеров, таких как Kubernetes и OpenShift. Они предоставляют механизм для организации и идентификации ресурсов в кластере, что облегчает управление, поиск и сегментацию ресурсов.  

**Labels** - это пары ключ-значение, которые можно присвоить объектам в кластере, таким как Pods, Services, ReplicaSets и другие. Лейблы не имеют предопределенного назначения и могут использоваться для определения характеристик, свойств или категорий объектов. Примеры лейблов: ```"environment=production"```, ```"app=web"```, ```"tier=frontend"```. Лейблы добавляют метаданные к объектам и позволяют классифицировать их по различным критериям.  

**Selectors** - это выражения, которые позволяют выбирать объекты на основе их лейблов. Селекторы используются для определения, какие объекты должны быть выбраны для выполнения определенных действий, таких как **балансировка нагрузки, размещение на узлах и т. д**. Селекторы могут быть созданы на основе конкретных лейблов и их значений. Например, селектор ```"environment=production"``` будет выбирать все объекты с лейблом ```"environment"``` и значением ```"production"```.  
Также селектор, может выбирать поды на основе разных лейблов. Пример: Предположим, у нас есть несколько подов, каждый из которых имеет следующие лейблы:  
* ```app=web``` - поды, отвечающие за веб-сервер.
* ```app=api``` - поды, отвечающие за сервер API.
* ```environment=production``` - поды, работающие в продакшн окружении.  
Пример селектора, который выбирает поды на основе разных лейблов:  
```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: example-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
      environment: production
```  
  
В этом примере selector выбирает только поды с лейблами ```app=web``` и ```environment=production```. То есть, **ReplicaSet** будет управлять репликами только тех подов, которые имеют оба эти лейбла одновременно.  
Это может быть полезно, например, когда мы хотим масштабировать только веб-сервера, работающие в production окружении, оставив другие типы подов без изменений.  
  
Таким образом, применение лейблов и селекторов включает:
* **Группировку и организацию:** Лейблы позволяют группировать и организовывать ресурсы в кластере, упрощая управление и поиск.
* **Балансировку нагрузки:** Селекторы используются для определения, на какие воркер-ноды размещать поды, обеспечивая балансировку нагрузки.
* **Управление ресурсами:** Селекторы позволяют определить, какие поды должны быть масштабированы или удалены в зависимости от текущей нагрузки.
* **Поддержание высокой доступности:** Лейблы и селекторы могут использоваться для размещения реплицированных подов на различных воркер-нодах для обеспечения высокой доступности.
* **Разграничение окружений:** Лейблы позволяют разграничить объекты по окружениям (например, test, staging, prodaction).
  
Использование лейблов и селекторов дает возможность более гибко и эффективно управлять и масштабировать ресурсы в кластере, а также упрощает автоматизацию и оркестрацию приложений.  

### Route
**Route** - это объект, который обеспечивает доступ к сервисам, работающим внутри кластера, извне кластера. **Route** позволяет настроить внешний доступ к нашим приложениям, предоставляя внешний URL (HTTP/HTTPS), который клиенты могут использовать для взаимодействия с нашими сервисами.  
  
**Основные характеристики и функции Route:**
* **Внешний доступ:** Route предоставляет механизм для доступа к сервисам внутри кластера через внешний URL, который можно использовать извне кластера, например, из Интернета.
* **Терминация SSL:** Route может обеспечивать терминацию SSL (шифрование) для безопасной передачи данных между клиентами и нашими сервисами.
* **Многоподдоменные маршруты:** Вы можете настроить множество доменов для одного маршрута, что полезно, когда у вас есть несколько поддоменов для разных частей приложения.
* **Интеграция с сервисами:** Route интегрируется с сервисами в нашем кластере, позволяя перенаправлять трафик к нужным сервисам.
* **Сервисное открытие:** Route позволяет открывать доступ к наших сервисам, даже если они находятся за NAT или внутри внутренней сети.
* **Обеспечение доступности:** Route обеспечивает балансировку нагрузки и высокую доступность, направляя трафик к нескольким экземплярам нашего сервиса.  
  
Таким образом **Route** - это важный инструмент для обеспечения доступности и безопасности нашего приложения, когда оно доступно извне кластера.  

**Важно понимать, что есть 2 сущности:**
1. **Ingress** - родная сущность от Kubernetes, которая также поддерживается OpenShift.
2. **Route** - сущность RedHat'а.  
В OpenShift рекомендуется всегда создавать роуты.  
Если создать **ingress** как сущность доступа вне кластера в кластер, то автоматически создастся сущность **route**. Т.е. создали одно, а получилось две - в этом случае лучше сразу создавать **route** без **ingress**.  
  
![Ingress](images/Ingress.PNG)  
  
Если мы хотим снаружи подключиться к нашему приложению (которое находится в pod, который находится в кластере, и к которому уже есть **Service**) - мы создаём **Route**.  
**Route** ведет на кластер OpenShift, где её слушает сущность **Service**, который смотрит на **selector pods**, тот смотрит какие поды есть, и отдаёт нам, например, Hello-World во всю страницу.  
  
**Route** можно создавать из меню **Routes**, но также это можно делать через **Ingresses**.  
  
**Важно!**
В рамках одного namespace (project), в рамках одного кластера, мы можем обращаться к соседним подам через **Service** (например, ```curl example```, где example это имя сервиса).  
А объекты типа **Routes** придуманы для того, чтобы мы вне кластера обращались к тому или иному поду или контейнеру. **Route** смотрит на сервис.  
Внутри контейнера мы можем обращаться как по имени **сервиса**, так и по имени **роута**.  
  
**Роуты** и **Ингрессы** могут использовать разные провайдеры, но чаще всего используются **HAproxy**. Можно использовать **Nginx**, можно другие на **baremetal**. Эти все провайдеры позволяют определить в себе настройки.
  
  
## Что происходит когда запускается под?
Первым делом когда мы либо мы присылаем yaml конфиг в API OpenShift, либо вводим команду, у нас происходит общение с API кластера, затем сам кластер обращается к **Kube Controller Manager**, который смотрит на работу всех нод кластера - worker nodes, master nodesЮ смотрит какие ноды доступны для того чтобы на них можно было завести поды, какие из нод содержат какие-нибудь селекторы и т.д. Т.е. происзводится множество проверок того, какая из нод нам подойдёт для того чтобы мы именно на неё задеплоили свой под.  
**Controller Manager** следит за состоянием нод, и если происходят какие-либо сбои, он об этом пишет в **events**, и проблемную ноду можно вывести из работы - так называемо задрейнить (drain a node), т.е. вывести её из работы кластера.  
  
После того как мы определились с нодой, далее **Scheduller** (процесс внутри master node) отслеживает создание пода, и командует той ноде, которая выбрана при помощи **Controller Manager** и проверяет множество факторов:  
* Минимальные рекомендуемые ресурсы, которые необходимы для ваших приложений (**requests** resources и **limits** resources по cpu и memory), это т.н. **Affinity принадлежности** (подробнее можно прочитать в статье https://habr.com/ru/company/otus/blog/576944/) к нодам.  
**Affinity-правила** позволяют нам контролировать, на каких узлах кластера будут размещаться наши поды. Эти правила определяют предпочтения и ограничения для размещения подов на определенных узлах с учетом их атрибутов, как, например, лейблы. Есть два типа affinity-правил: **node affinity** (аффинити узлов) и **pod affinity** (аффинити подов):
  * **Node Affinity (Аффинити Узлов):** С помощью **node affinity** можно настраивать размещение подов на определенных узлах кластера на основе атрибутов этих узлов. Например, вы можете указать, что поды должны запускаться только на узлах с определенными лейблами или аннотациями (**Аннотации** - это метаданные, которые могут быть прикреплены к различным ресурсам, включая pods, services, nodes и другие объекты. Они представляют собой пары ключ-значение и используются для добавления дополнительной информации о ресурсе, которая может быть полезной для различных целей, таких как отладка, мониторинг, анализ и т.д. Прописываются в блоке с метаданными). Пример:  
  ```
  affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: disk-type
            operator: In
            values:
            - ssd
  ```
    * **Pod Affinity (Аффинити Подов):** С помощью **pod affinity** можно контролировать размещение подов на одних узлах вместе с другими подами, которые имеют определенные атрибуты (лейблы или аннотации). То есть можно указать, что определенные поды следует размещать на тех же узлах, где уже есть поды с определенными атрибутами. Пример:  
    ```
    affinity:
  podAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
            - key: app
              operator: In
              values:
                - web
        topologyKey: "kubernetes.io/hostname"
    ```  
    Оба эти типа аффинити-правил позволяют более точно контролировать размещение подов на узлах в зависимости от их атрибутов. Это может быть полезно для управления нагрузкой, группировки связанных подов на одних узлах или разделения нагрузки между разными типами узлов.
* Использование жёстких дисков
* Всякие лейблы и селекторы

## Probes
**Probes** (или **healthchecks**) в OpenShift и Kubernetes - это механизмы для проверки здоровья контейнеров, запущенных в подах. Они позволяют системе автоматически определять, насколько работоспособен и готов к обработке запросов контейнер.  
    
**Probes** выполняются с определенной периодичностью и с использованием определенной команды или запроса. Если результат проверки соответствует заданным условиям (например, успешный HTTP-запрос с кодом 200), то под считается здоровым. В противном случае, в зависимости от типа проверки, контейнер может быть перезапущен или отключен от балансировщика нагрузки.  

**Существуют три типа probes:**
* **Startup probes**
Это экспериментальная проверка, которая используется для задания времени запуска контейнера. Она может быть полезна, когда контейнер нуждается в длительной инициализации.
* **liveness probes**
Этот тип проверки определяет, готов ли контейнер обрабатывать запросы. Если liveness probe не проходит, то контейнер перезапускается.  
Пример определения проверки здоровья в readiness probe в YAML-файле пода:  
```
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image
      livenessProbe:
        httpGet:                 # Указывает, что для проверки здоровья будет выполнен HTTP-запрос
          path: /healthz         # Путь, по которому будет выполняться запрос
          port: 8080 # Номер порта контейнера для запроса
        initialDelaySeconds: 10  # начальная задержка перед началом выполнения проверок здоровья (10 секунд после запуска контейнера)
        periodSeconds: 15        # периодичность выполнения проверок здоровья (каждые 15 секунд после начала выполнения), но после того как прошла начальная задержка initialDelaySeconds
```
* **readiness probes**
Этот тип проверки определяет, готов ли контейнер принимать трафик. Если под не прошел readiness probe, его изолируют от балансировщика нагрузки.  
Пример определения проверки здоровья в readiness probe в YAML-файле пода:  
```
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: my-image
      readinessProbe:
        httpGet:                 # Указывает, что для проверки здоровья будет выполнен HTTP-запрос
          path: /healthz         # Путь, по которому будет выполняться запрос
          port: 8080             # Номер порта контейнера для запроса
        initialDelaySeconds: 5   # Начальная задержка в секундах. Определяет, сколько времени должно пройти с момента запуска контейнера до того, как начнут выполняться проверки здоровья. Может быть полезна, чтобы дать контейнеру достаточно времени для запуска и инициализации перед тем, как начать проверки
        periodSeconds: 10        # Период проверки в секундах. Определяет, с какой периодичностью будут выполняться проверки здоровья после того, как начнут работу (то, как часто будут выполняться проверки здоровья после начала выполнения контейнера, но после того как прошла начальная задержка initialDelaySeconds)
```  
  
#### При запуске пода стартуют 3 типа контейнеров:
* **Init container**
**Init container** не отображается, но он запускается и ждёт пока этому контейнеру выдадут сеть, чтобы и **kubelet** и **kubeproxy** могли взаимодействовать с этим контейнером. Т.е. этот контейнер поднимается, получает сеть, выдаёт нашему поду и самоуничтожается.  
Если же он не получит адреса, мы увидим сообщение об этом в **events**, например о том, что закончились ip адреса.  
Они предназначены для выполнения предварительных задач или инициализации перед запуском основных контейнеров в поде. Init-контейнеры могут выполнять скрипты инициализации, загружать данные, устанавливать зависимости и выполнять другие действия, которые необходимо выполнить перед началом работы основных контейнеров.  
Init-контейнеры запускаются перед основными контейнерами в поде и выполняются только один раз до завершения. Их задача - подготовить окружение для основных контейнеров.  

* **Pause container** 
**Pause container** (также иногда называемый **infra контейнером**) - это особый служебный контейнер, который существует в каждом поде, это похоже на шаблон или на родительский контейнер, от которых все новые контейнеры в поде наследуют namespaces. Контейнер pause запускается, затем переходит в “спящий режим”. Это контейнер-шаблон, который резервирует namespaces, которые являются общими для всех контейнеров внутри пода.  
Используется внутри подов для организации сетевой изоляции и управления сетевыми пространствами имен.  
Один из способов обеспечения сетевой изоляции между контейнерами внутри одного пода - это использование контейнера Pause. Контейнер Pause создает сетевое пространство имен и сетевой интерфейс, а затем делегирует сетевые пространства имен остальным контейнерам в поде. Это позволяет каждому контейнеру иметь свой уникальный сетевой стек, но все контейнеры в поде могут взаимодействовать друг с другом через localhost.  
Важно понимать, что контейнер Pause сам по себе не выполняет никаких задач и не взаимодействует с приложением, размещенным в поде. Его основная функция - обеспечить сетевую изоляцию между контейнерами в поде.  
Этот подход позволяет более эффективно использовать ресурсы хостовой системы и предоставлять приложениям внутри одного пода уровень изоляции, аналогичный изоляции, которая была бы между разными подами.  
Pause контейнер запускается вместе с основными контейнерами и продолжает работать в фоновом режиме, обеспечивая сетевую изоляцию между контейнерами в поде.  
Когда стартует **Pause container**, он нам арендует ip, и далее стартует наш под.  
  
* **Основные (Main) контейнеры**  
Это контейнеры, которые выполняют основные задачи нашего приложения. Они могут быть веб-серверами, микросервисами или любыми другими компонентами нашего приложения, которые предоставляют функциональность пользователям или системе.


**Таким образом в нашем поде есть 3 типа контейнеров, которые стартуют в следующей последовательности:**
1. **Init container** - используются для каких-то вещей, которые нужно выполнить до старта нашего основного приложения - например, чтобы запустилась БД, чтобы фронт был доступен. Однако чаще всего это используется для того чтобы из vault'а вытащить логины и пароли, помещает их в переменные окружения и передаёт нашему основному контейнеру. Данный тип контейнеров запускается раньше всех - раньше нашего основного (main), раньше сайдакара (**sidecar** - шаблон проектирования и архитектурный паттерн, который предполагает добавление дополнительного контейнера к основному контейнеру в одном поде. Основная идея состоит в том, чтобы разделить разные функции или задачи приложения на отдельные контейнеры, которые могут работать параллельно в рамках одного пода. **Sidecar-контейнеры** обычно взаимодействуют с основным контейнером и предоставляют дополнительную функциональность или сервисы. Они могут выполнять задачи, такие как сбор логов, мониторинг, авторизация, сетевые прокси, обновление данных и другие. Каждый sidecar-контейнер работает независимо от основного контейнера, но может использовать общие ресурсы и обмениваться данными с другими контейнерами в том же поде).  
Выполняются только один раз до их завершения и самоуничтожаются. Их задача - подготовить окружение для основных контейнеров.  
2. **Main (основные) контейнеры** - после успешного завершения всех init-контейнеров, параллельно запускаются основные контейнеры (main). Эти контейнеры представляют собой основную рабочую нагрузку приложения. 
3. **Pause container** - После запуска основных контейнеров, OpenShift/Kubernetes также создает pause container, который предоставляет сетевое пространство имен и сетевой интерфейс для обеспечения изоляции сетевого взаимодействия между контейнерами в поде - получает и предоставляет IP.  
Pause контейнер запускается вместе с основными контейнерами и продолжает работать в фоновом режиме, обеспечивая сетевую изоляцию между контейнерами в поде.  
  
## Стратегии деплоя (deployment strategies)
Подробно в документации https://docs.openshift.com/container-platform/3.11/dev_guide/deployments/deployment_strategies.html  
**Существуют различные стратегии деплоя:**  
* **RollingUpdate** (```spec.strategy.type: RollingUpdate```) - это то что используется в основном. Описывается в объектах Deployment или DeploymentConfig, и она означает, что - мы говорим Kubernetes/OpenShift, что сперва у нас поднимаются новые поды, проходят все проверки, проходит трафик, а потом гасятся старые. В случае с этим типом стратегии даунтайм минимальный.  
* **Recreate** - когда сразу удаляется старый под и поднимается новый. При этом типе стратегии будет даунтайм.
* **Bluegreen** и **конарейка (конареечный релиз)**
**Bluegreen** - часть подов поднимаются с одним образом (новым), а часть с другим (старым). Это может быть полезно, когда лишняя секунда простоя будет фатальной для бизнеса. Т.е. частично деплоить и постепенно увеличивать кол-во экземпляров с новой версией образа.  
**Bluegreen** и **конарейка** обычно реализуются при помощи bash, python и прочих "костылей". Не является функцией по умолчанию.  
**Bluegreen** нецелесобразно использовать в архитектуре, где также требуется обновить БД, т.к. структура БД меняется при каждом релизе, и если 2 реплики будут работать с одной структурой, а 2 с другой, у нас всё развалится. В тех проектах где БД нет, или нет привязки к структуре, не нужна **миграция liquibase**, это сработает.
